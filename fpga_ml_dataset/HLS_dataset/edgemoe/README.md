The contents of this directory are based on the [sharc-lab/Edge-MoE](https://github.com/sharc-lab/Edge-MoE) repository at commit [`a214ade`](https://github.com/sharc-lab/Edge-MoE/tree/a214ade17c59408f47afbb4c0cc7db05098f1888).

---

# Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts

Rishov Sarkar<sup>1</sup>, Hanxue Liang<sup>2</sup>, Zhiwen Fan<sup>2</sup>, Zhangyang Wang<sup>2</sup>, Cong Hao<sup>1</sup>

<sup>1</sup>School of Electrical and Computer Engineering, Georgia Institute of Technology  
<sup>2</sup>School of Electrical and Computer Engineering, University of Texas at Austin

## Overview

![Edge-MoE overall architecture](https://github.com/sharc-lab/Edge-MoE/blob/a214ade17c59408f47afbb4c0cc7db05098f1888/images/edge-moe-arch.svg)

This is **Edge-MoE**, the *first end-to-end* FPGA accelerator for *multi-task ViT* with a rich collection of architectural innovations.

Currently, this repository contains a prebuilt bitstream for the AMD/Xilinx ZCU102 FPGA and a video demo. Our HLS code will be open-source upon acceptance.
